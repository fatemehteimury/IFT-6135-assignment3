{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Assignment3_problem2.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"sHlzFmVOFAv2","colab_type":"code","colab":{}},"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","\n","import torch.distributions as distribution\n","import math\n","\n","from torchvision.datasets import utils\n","import torch.utils.data as data_utils\n","import torch\n","import os\n","import numpy as np\n","from torch import nn\n","from torch.nn.modules import upsampling\n","from torch.functional import F\n","from torch.optim import Adam"],"execution_count":0,"outputs":[]},{"metadata":{"id":"U7sPhfHIFM0t","colab_type":"code","colab":{}},"cell_type":"code","source":["# Device configuration\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"iDk8TgZB936H","colab_type":"code","colab":{}},"cell_type":"code","source":["batch_size = 32\n","learning_rate = 3e-4\n","epochs = 20\n","latent_variable_dim = 100"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Mq9pQQY7aFI_","colab_type":"code","colab":{}},"cell_type":"code","source":["#Code provided to load the data\n","\n","def get_data_loader(dataset_location, batch_size):\n","    URL = \"http://www.cs.toronto.edu/~larocheh/public/datasets/binarized_mnist/\"\n","    \n","    # start processing\n","    def lines_to_np_array(lines):\n","        return np.array([[int(i) for i in line.split()] for line in lines])\n","    splitdata = []\n","    \n","    for splitname in [\"train\", \"valid\", \"test\"]:\n","        filename = \"binarized_mnist_%s.amat\" % splitname\n","        filepath = os.path.join(dataset_location, filename)\n","        utils.download_url(URL + filename, dataset_location)\n","        with open(filepath) as f:\n","            lines = f.readlines()\n","        x = lines_to_np_array(lines).astype('float32')\n","        x = x.reshape(x.shape[0], 1, 28, 28)\n","        \n","        # pytorch data loader\n","        dataset = data_utils.TensorDataset(torch.from_numpy(x))\n","        dataset_loader = data_utils.DataLoader(x, batch_size = batch_size, shuffle = splitname == \"train\")\n","        splitdata.append(dataset_loader)\n","        splitdata.append(dataset)\n","    return splitdata"],"execution_count":0,"outputs":[]},{"metadata":{"id":"KhbluRvHaIat","colab_type":"code","outputId":"255612d3-9804-461d-8c2e-63dfc1ea35af","executionInfo":{"status":"ok","timestamp":1555984245130,"user_tz":240,"elapsed":17204,"user":{"displayName":"Kevin Pham","photoUrl":"","userId":"17463352136465305825"}},"colab":{"base_uri":"https://localhost:8080/","height":67}},"cell_type":"code","source":["trainloader, train, validloader, valid, testloader, test = get_data_loader(\"binarized_mnist\", batch_size)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Using downloaded and verified file: binarized_mnist/binarized_mnist_train.amat\n","Using downloaded and verified file: binarized_mnist/binarized_mnist_valid.amat\n","Using downloaded and verified file: binarized_mnist/binarized_mnist_test.amat\n"],"name":"stdout"}]},{"metadata":{"id":"ZHUu4bLLfmMK","colab_type":"text"},"cell_type":"markdown","source":["We use the tutorial provided in the following websites to help us implement our VAE :\n","http://hameddaily.blogspot.com/2018/12/yet-another-tutorial-on-variational.html\n","and https://github.com/pytorch/examples/blob/master/vae/main.py"]},{"metadata":{"id":"sqMC8wPDfDta","colab_type":"text"},"cell_type":"markdown","source":["**Train a VAE**"]},{"metadata":{"id":"sUTwBhDcZYnG","colab_type":"code","colab":{}},"cell_type":"code","source":["class VAE(nn.Module):\n","    def __init__(self):\n","        super(VAE, self).__init__()\n","\n","        # For encoder\n","        self.fcE1   = nn.Linear(in_features=256, out_features=2*latent_variable_dim)\n","        self.convE1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1)\n","        self.convE2 = nn.Conv2d(in_channels=32 , out_channels=64 , kernel_size=3)\n","        self.convE3 = nn.Conv2d(in_channels=64 , out_channels=256, kernel_size=5)\n","                \n","        # For decoder\n","        self.fcD1   = nn.Linear(in_features=latent_variable_dim, out_features=256)\n","        self.convD1 = nn.Conv2d(in_channels=256, out_channels=64, kernel_size=5, padding=4)\n","        self.convD2 = nn.Conv2d(in_channels=64 , out_channels=32, kernel_size=3, padding=2)\n","        self.convD3 = nn.Conv2d(in_channels=32 , out_channels=16, kernel_size=3, padding=2)        \n","        self.convD4 = nn.Conv2d(in_channels=16 , out_channels=1 , kernel_size=3, padding=2)\n","        self.upsampling = nn.UpsamplingBilinear2d(scale_factor=2)\n","\n","    def encode(self, x):\n","        h1 = F.elu(self.convE1(x))\n","        h2 = F.avg_pool2d(h1, kernel_size=2, stride=2)\n","        h3 = F.elu(self.convE2(h2))\n","        h4 = F.avg_pool2d(h3, kernel_size=2, stride=2)\n","        h5 = F.elu(self.convE3(h4))\n","        h5 = h5.reshape(-1,256)\n","        h6 = self.fcE1(h5) \n","        mu = h6[:,:latent_variable_dim]\n","        logvar = h6[:,latent_variable_dim:]\n","        return  mu , logvar\n","\n","    def reparameterize(self, mu, logvar):\n","        std = torch.exp(0.5*logvar)\n","        eps = torch.randn_like(std)\n","        return mu + eps*std\n","\n","    def decode(self, z):\n","        h7 = F.elu(self.fcD1(z))\n","        h7 = h7.reshape(z.shape[0], 256, 1, 1)\n","        h8 = F.elu(self.convD1(h7))\n","        h9 = self.upsampling(h8)\n","        h10 = F.elu(self.convD2(h9))\n","        h11 = self.upsampling(h10)\n","        h12 = F.elu(self.convD3(h11))\n","        return torch.sigmoid(self.convD4(h12))\n","\n","    def forward(self, x):\n","        mu, logvar = self.encode(x)\n","        z = self.reparameterize(mu, logvar)\n","        return self.decode(z), mu, logvar\n","      \n","    def get_sample(self, mu, logvar, x):\n","        epsilon = torch.randn(x.shape[0], latent_variable_dim).to(device)\n","        return epsilon * (logvar/2).exp() + mu \n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"e_L28FEtetdh","colab_type":"code","colab":{}},"cell_type":"code","source":["model = VAE()\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","optimizer = Adam(model.parameters(), lr=learning_rate)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"jTvX-SeFgdue","colab_type":"code","colab":{}},"cell_type":"code","source":["def loss_function(recon_x, x, mu, logvar):\n","    BCE = F.binary_cross_entropy(recon_x.view(-1, 784), x.view(-1, 784), reduction='sum')\n","    KLD = 0.5 * torch.sum(-1 - logvar + mu**2 + logvar.exp())\n","\n","    return BCE + KLD"],"execution_count":0,"outputs":[]},{"metadata":{"id":"mccSgL-eefZN","colab_type":"code","outputId":"9c3ff7ad-f006-4b58-fc43-ab363e44a4cf","executionInfo":{"status":"ok","timestamp":1555984598783,"user_tz":240,"elapsed":370584,"user":{"displayName":"Kevin Pham","photoUrl":"","userId":"17463352136465305825"}},"colab":{"base_uri":"https://localhost:8080/","height":386}},"cell_type":"code","source":["log_interval = batch_size\n","\n","for epoch in range(epochs):\n","    model.train()\n","    train_loss = 0\n","    for i, data in enumerate(trainloader):\n","        data = data.to(device)\n","        optimizer.zero_grad()\n","        recon_batch, mu, logvar = model(data)\n","        loss = loss_function(recon_batch, data, mu, logvar)\n","        loss.backward()\n","        train_loss += loss.item()\n","        optimizer.step()\n","      \n","    model.eval()\n","    valid_loss = 0\n","    with torch.no_grad():\n","        for i, data in enumerate(validloader):\n","            data = data.to(device)\n","            recon_batch, mu, logvar = model(data)\n","            valid_loss += loss_function(recon_batch, data, mu, logvar).item()\n","            \n","    model.eval()\n","    test_loss = 0\n","    with torch.no_grad():\n","        for i, data in enumerate(testloader):\n","            data = data.to(device)\n","            recon_batch, mu, logvar = model(data)\n","            test_loss += loss_function(recon_batch, data, mu, logvar).item()\n","    \n","    print('Epoch: {} --- ELBO Train: {:.2f} ---  ELBO Validation: {:.2f} ---  ELBO Test: {:.2f}'.format(epoch + 1, -train_loss / len(trainloader.dataset), \n","                                                                                                        -valid_loss / len(validloader.dataset), -test_loss / len(testloader.dataset) ))\n","    "],"execution_count":9,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/upsampling.py:129: UserWarning: nn.UpsamplingBilinear2d is deprecated. Use nn.functional.interpolate instead.\n","  warnings.warn(\"nn.{} is deprecated. Use nn.functional.interpolate instead.\".format(self.name))\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 1 --- ELBO Train: -157.37 ---  ELBO Validation: -121.96 ---  ELBO Test: -120.69\n","Epoch: 2 --- ELBO Train: -114.38 ---  ELBO Validation: -109.88 ---  ELBO Test: -108.72\n","Epoch: 3 --- ELBO Train: -106.48 ---  ELBO Validation: -104.90 ---  ELBO Test: -103.84\n","Epoch: 4 --- ELBO Train: -102.78 ---  ELBO Validation: -101.42 ---  ELBO Test: -100.45\n","Epoch: 5 --- ELBO Train: -100.57 ---  ELBO Validation: -100.49 ---  ELBO Test: -99.48\n","Epoch: 6 --- ELBO Train: -99.15 ---  ELBO Validation: -98.85 ---  ELBO Test: -97.99\n","Epoch: 7 --- ELBO Train: -98.10 ---  ELBO Validation: -98.06 ---  ELBO Test: -97.31\n","Epoch: 8 --- ELBO Train: -97.25 ---  ELBO Validation: -97.21 ---  ELBO Test: -96.44\n","Epoch: 9 --- ELBO Train: -96.63 ---  ELBO Validation: -96.76 ---  ELBO Test: -96.05\n","Epoch: 10 --- ELBO Train: -96.01 ---  ELBO Validation: -96.18 ---  ELBO Test: -95.38\n","Epoch: 11 --- ELBO Train: -95.56 ---  ELBO Validation: -95.79 ---  ELBO Test: -94.89\n","Epoch: 12 --- ELBO Train: -95.15 ---  ELBO Validation: -95.29 ---  ELBO Test: -94.60\n","Epoch: 13 --- ELBO Train: -94.81 ---  ELBO Validation: -95.64 ---  ELBO Test: -94.82\n","Epoch: 14 --- ELBO Train: -94.46 ---  ELBO Validation: -95.26 ---  ELBO Test: -94.62\n","Epoch: 15 --- ELBO Train: -94.12 ---  ELBO Validation: -94.36 ---  ELBO Test: -93.80\n","Epoch: 16 --- ELBO Train: -93.80 ---  ELBO Validation: -94.41 ---  ELBO Test: -93.68\n","Epoch: 17 --- ELBO Train: -93.60 ---  ELBO Validation: -94.13 ---  ELBO Test: -93.54\n","Epoch: 18 --- ELBO Train: -93.33 ---  ELBO Validation: -93.92 ---  ELBO Test: -93.19\n","Epoch: 19 --- ELBO Train: -93.10 ---  ELBO Validation: -93.68 ---  ELBO Test: -92.99\n","Epoch: 20 --- ELBO Train: -92.95 ---  ELBO Validation: -93.75 ---  ELBO Test: -93.02\n"],"name":"stdout"}]},{"metadata":{"id":"JbNKxsg4fOcL","colab_type":"text"},"cell_type":"markdown","source":["**Evaluating log-likelihood with Variational Autoencoders **"]},{"metadata":{"id":"kUDQBvhAJBG7","colab_type":"code","colab":{}},"cell_type":"code","source":["def log_likelihood(model, data, M, K=200, D=784, L=100):\n","    with torch.no_grad():\n","        data = torch.utils.data.DataLoader(data, batch_size=M, shuffle=False)\n","        data = list(data)[0][0].to(device)\n","        mu, logvar = model.encode(data)\n","        \n","        normal_distribution = distribution.Normal(mu, (logvar/2).exp())\n","        std_normal_distribution = distribution.Normal(torch.zeros(L).to(device), torch.ones(L).to(device))\n","\n","        sum_p = []\n","        for i in range(K):\n","            z = model.get_sample(mu, logvar, data)\n","            recon_data = model.decode(z)\n","            log_p_z = torch.sum(std_normal_distribution.log_prob(z), 1)\n","            log_p_zx = torch.sum(normal_distribution.log_prob(z), 1)\n","            log_p_xz = -F.binary_cross_entropy(recon_data, data, reduction=\"none\").view(M, -1)\n","            log_p_xz = torch.sum(log_p_xz, 1)\n","            sum_p.append(log_p_xz + log_p_z - log_p_zx - math.log(K))\n","            \n","        log_p_x = torch.logsumexp(torch.stack(sum_p).to(device),0).cpu().numpy()\n","    return log_p_x"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Dk2lnG4pKt10","colab_type":"code","outputId":"12ac3ffc-c3d6-4e02-d5f4-519b1197078c","executionInfo":{"status":"ok","timestamp":1555984599134,"user_tz":240,"elapsed":370861,"user":{"displayName":"Kevin Pham","photoUrl":"","userId":"17463352136465305825"}},"colab":{"base_uri":"https://localhost:8080/","height":67}},"cell_type":"code","source":["val_log_likelihood = np.mean(log_likelihood(model, valid, M=batch_size))\n","print(\"Log-likelihood  estimate on Validation: {:.2f}\".format(val_log_likelihood))"],"execution_count":11,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/upsampling.py:129: UserWarning: nn.UpsamplingBilinear2d is deprecated. Use nn.functional.interpolate instead.\n","  warnings.warn(\"nn.{} is deprecated. Use nn.functional.interpolate instead.\".format(self.name))\n"],"name":"stderr"},{"output_type":"stream","text":["Log-likelihood  estimate on Validation: -83.33\n"],"name":"stdout"}]},{"metadata":{"id":"Xm1ExVz10qd0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":67},"outputId":"709c5c4f-7e27-4867-d715-20d3fc8696c3","executionInfo":{"status":"ok","timestamp":1555984599483,"user_tz":240,"elapsed":371176,"user":{"displayName":"Kevin Pham","photoUrl":"","userId":"17463352136465305825"}}},"cell_type":"code","source":["test_log_likelihood = np.mean(log_likelihood(model, test, M=batch_size))\n","print(\"Log-likelihood  estimate on Test: {:.2f}\".format(test_log_likelihood))"],"execution_count":12,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/upsampling.py:129: UserWarning: nn.UpsamplingBilinear2d is deprecated. Use nn.functional.interpolate instead.\n","  warnings.warn(\"nn.{} is deprecated. Use nn.functional.interpolate instead.\".format(self.name))\n"],"name":"stderr"},{"output_type":"stream","text":["Log-likelihood  estimate on Test: -95.09\n"],"name":"stdout"}]}]}